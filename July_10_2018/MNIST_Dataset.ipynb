{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using advanced techniques for training neural network:\n",
    "* Weight Initialization\n",
    "* Nonlinearity (Activation Function)\n",
    "* Optimizers\n",
    "* Batch Normalization\n",
    "* Dropout (Regularization)\n",
    "* Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST database of handwritten digits\n",
    "# into training and testing dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n8DZI6NXofNrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131123a5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    }
   ],
   "source": [
    "# show first number in the dataset\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "print('Label: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4Ii/EBQhB8IivADQRF+ICjCDwT1fwW0VTLgp/lVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1311d4222b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n"
     ]
    }
   ],
   "source": [
    "# show tenth number in the dataset\n",
    "plt.imshow(X_test[10])\n",
    "plt.show()\n",
    "print('Label: ',y_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshaping X data (features): (n, 28, 28) => (n, 784)\n",
    "# Input image is a colorful image, 3 channel(RGB),\n",
    "# while the network expects a gray image, one channel\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting the dataset \n",
    "# using 33% of the training data to expedite the training process\n",
    "X_train, _ , y_train, _ = train_test_split(X_train, y_train, test_size = 0.67, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting y data into categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (19800, 784)\n",
      "X_test:  (10000, 784)\n",
      "y_train:  (19800, 10)\n",
      "y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# looking at our reformed dataset\n",
    "print('X_train: ',X_train.shape)\n",
    "print('X_test: ',X_test.shape)\n",
    "print('y_train: ',y_train.shape)\n",
    "print('y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create basic MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add layers\n",
    "model.add(Dense(50, input_shape= (784,), activation = 'sigmoid'))\n",
    "model.add(Dense(50, activation = 'sigmoid'))\n",
    "model.add(Dense(50, activation = 'sigmoid'))\n",
    "model.add(Dense(50, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model compiling\n",
    "sgd = optimizers.SGD(lr = 0.001) # stochastic gradient descent optimizer\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 47,410\n",
      "Trainable params: 47,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summry of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. for ```dense_1``` layer: no. of param(39250) = no. of input values(784) * neurons in first layer(50) + bias value(50)\n",
    "2. for ```dense_2``` layer: no. of param(2550) = no. of input values(50) * neurons in second layer(50) + bias value(50)\n",
    "3. for ```dense_3``` layer: no. of param(2550) = no. of input values(50) * neurons in third layer(50) + bias value(50)\n",
    "4. for ```dense_4``` layer: no. of param(2550) = no. of input values(50) * neurons in fourth layer(50) + bias value(50)\n",
    "5. for ```dense_5``` layer: no. of param(510) = no. of input values(50) * neurons in last layer(10) + bias value(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model\n",
    "##### one epoch = \n",
    "one forward pass and one backward pass of all the training examples\n",
    "##### batch size = \n",
    "the number of training examples in one forward/backward pass. The hgher the batch size the more memory space you'll need\n",
    "##### Example: \n",
    "If you have 1000 training examples, and your batch size is 500, then it takes 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size = 256, validation_split = 0.3, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Conclusion:\n",
    "1. Parameters Used: <b>```Sigmoid```</b> activation function at input layers and <b>```Softmax```</b> activation function at output layers.\n",
    "2. Accuracy on Testing Dataset is <b>```11.35%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Weight Initialization\n",
    "* Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree\n",
    "* Initializations define the way to set the initial random weights of keras layers.\n",
    "* He normal initializer draws samples from a truncated normal distribution centered on 0 with ```stddev = sqrt(2 / fan_in)``` where ```fan_in``` is the number of input units in the weight tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to generate (return) models\n",
    "\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape = (784, ), kernel_initializer = 'he_normal', activation= 'sigmoid'))\n",
    "    model.add(Dense(50, kernel_initializer= 'he_normal', activation= 'sigmoid'))\n",
    "    model.add(Dense(50, kernel_initializer= 'he_normal', activation= 'sigmoid'))\n",
    "    model.add(Dense(50, kernel_initializer= 'he_normal', activation= 'sigmoid'))\n",
    "    model.add(Dense(10, kernel_initializer= 'he_normal', activation= 'softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer= sgd, loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 47,410\n",
      "Trainable params: 47,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verbose can be 0, 1 or 2\n",
    "1. verbose= 0: will show you nothing\n",
    "2. verbose= 1: will show you animated progress bar \n",
    "3. verbose= 2: will just mention the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13119ce4b00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split= 0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7552\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. Parameters Used: <b>```Sigmoid```</b> activation function at input layers, <b>```Softmax```</b> activation function at output layer, and <b>```he_normal```</b> as the kernel initializer.\n",
    "2. Accuracy on testing dataset is <b>```75.52%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Nonlinearity(Activation function)\n",
    "* Sigmoid functions suffer from gradient vanishing problem, making training slower.\n",
    "* Choices apart from sigmoid and tanh are:\n",
    "  * <b>```relu```</b>(rectified linear unit)\n",
    "  * <b>```selu```</b>(scaled exponential linear unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to generate (return) model\n",
    "def mlp_nonlinear_model():\n",
    "    model = Sequential()\n",
    "    # adding layers\n",
    "    model.add(Dense(50, input_shape= (784, ), activation= 'relu'))\n",
    "    model.add(Dense(50, activation= 'relu'))\n",
    "    model.add(Dense(50, activation= 'relu'))\n",
    "    model.add(Dense(50, activation= 'relu'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr= 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13119deb828>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp_nonlinear_model()\n",
    "model.fit(X_train, y_train, validation_split= 0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9288\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>```Parameters Used```</b>: <b>Relu</b> as activation function for input layers, and <b>Softmax</b> as activation function for output layer.\n",
    "2. Accuracy on testing dataset is <b>```92.88%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Optimizers (Adam:Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_adam_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape= (784, ), activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    \n",
    "    adam= optimizers.Adam(lr= 0.001)\n",
    "    model.compile(optimizer= adam, loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1311a364e80>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp_adam_model()\n",
    "model.fit(X_train, y_train, validation_split= 0.3, epochs= 100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9259\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Sigmoid```</b> activation for input layers, <b>```Softmax```</b> activation function for output layer, and <b>```Adam```</b> as an optimizer.\n",
    "2. Accuracy on testing datset is <b>```92.59%```</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using Adam as optimizer and Relu as activation function\n",
    "\n",
    "def mlp_adam_relu_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape= (784, ), activation= 'relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr= 0.001)\n",
    "    model.compile(optimizer = adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1311aab4908>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp_adam_relu_model()\n",
    "model.fit(X_train, y_train, validation_split= 0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_accuracy:  0.9385\n"
     ]
    }
   ],
   "source": [
    "print('Test_accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Relu```</b> as an activation function for input layers, <b>```Softmax```</b> as an activation function for output layer, and <b>```Adam```</b> as an optimizer.\n",
    "2. Accuracy on testing dataset is <b>```93.85%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Batch Normalization\n",
    "1. It is one of the method to prevent the \"internal covariance shift\" problem, has proven to be highly effective.\n",
    "2. Normalize each mini-batch before nonlinearity\n",
    "3. Batch Normalization layer is usually inserted after dense/convolution and before nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_batch_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape= (784, )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sdg = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer= sgd, loss= 'categorical_crossentropy', metric=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13120906c18>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 62us/step\n"
     ]
    }
   ],
   "source": [
    "results= model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_accuracy:  0.9546\n"
     ]
    }
   ],
   "source": [
    "print('Test_accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Sigmoid```</b> as an activation function for input layers, and <b>```Softmax```</b> as an activation function for output layer, and using <b>```BatchNormalization```</b> after dense and before activation.\n",
    "2. Accuracy on testing dataset is <b>```95.46%```</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using BatchNormalization along with relu as an activation function\n",
    "\n",
    "def mlp_batch_relu_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape=(784, )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr= 0.01)\n",
    "    model.compile(optimizer= sgd, loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131253f2320>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp_batch_relu_model()\n",
    "model.fit(X_train, y_train, validation_split= 0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 78us/step\n"
     ]
    }
   ],
   "source": [
    "results= model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_accurracy:  0.9471\n"
     ]
    }
   ],
   "source": [
    "print('Test_accurracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Relu```</b> as an activation function for input layers, and <b>```Softmax```</b> as an activation function for output layer, and using <b>```BatchNormalization```</b> after dense and before activation layer.\n",
    "2. Accuracy on testing dataset is <b>```94.71%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Dropout (Regularization)\n",
    "1. Dropout is one of the pwerful ways to prevent overfitting.\n",
    "2. The idea is to disconnecting some(randomly selected) neurons in each layer\n",
    "3. Dropout Rate is the probability of each neuron to be disconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_dropout_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape=(784, ), activation= 'sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    \n",
    "    sdg = optimizers.SGD(lr= 0.001)\n",
    "    \n",
    "    model.compile(optimizer = sdg, loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131230487f0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp_dropout_model()\n",
    "model.fit(X_train, y_train, validation_split = 0.3, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 71us/step\n"
     ]
    }
   ],
   "source": [
    "results= model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "print('Test_accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Sigmoid```</b> as an activation function for input layers, and <b>```Softmax```</b> as an activation function for output layer, and using <b>```Dropout```</b> after activation layer.\n",
    "2. Accuracy on testing dataset is <b>```11.35%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Ensemble\n",
    "1. It is a reliable and promising way to boost performance of the model\n",
    "2. Usually create 8 to 10 independent networks and merge their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_ensemble_model():\n",
    "    model= Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape=(784, ), activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(50, activation= 'sigmoid'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer= sgd, loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = KerasClassifier(build_fn= mlp_ensemble_model, epochs= 100, verbose= 0)\n",
    "model2 = KerasClassifier(build_fn= mlp_ensemble_model, epochs= 100, verbose= 0)\n",
    "model3 = KerasClassifier(build_fn= mlp_ensemble_model, epochs= 100, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_clf = VotingClassifier(estimators = [('model1', model1), ('model2', model2), ('model3', model3)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('model1', <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001312DEB94E0>), ('model2', <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001312DEB9550>), ('model3', <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001312DEB95C0>)],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praneet\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = ensemble_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.2793\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "1. <b>Parameters Used</b>: <b>```Sigmoid```</b> as an activation function for input layers, and <b>```Softmax```</b> as an activation function for output layer, and using <b>```Model Ensemble```</b>.\n",
    "2. Accuracy on testing dataset is <b>```27.93%```</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Model         | Testing Accuracy (%) | \n",
    "| ------------- |:----------------:    | \n",
    "| Basic Model   | 11.35                | \n",
    "| He Normal     | 75.52                | \n",
    "| Relu          | 92.88                | \n",
    "| Adam          | 92.59                | \n",
    "| Adam & Relu   | 93.85                | \n",
    "| Batchnorm     | 95.46                |\n",
    "| Batchnorm & Relu | 94.71                |\n",
    "| Dropout       | 11.35                |\n",
    "| Ensemble      | 27.93                |\n",
    "\n",
    "### Note : \n",
    "##### Most methods improve the model training & test performance. That's why we will use them all together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
